# generated from buildfarm_perf_tests/test/test_pub_sub.py.in
# generated code does not contain a copyright notice

from glob import glob
import os
import sys
import tempfile
import unittest

from buildfarm_perf_tests.launch import assert_wait_for_successful_exit
from buildfarm_perf_tests.launch import SystemMetricCollector
from buildfarm_perf_tests.test_results import read_performance_test_csv
from buildfarm_perf_tests.test_results import write_jenkins_benchmark_json
from buildfarm_perf_tests.test_results import write_jenkins_plot_csv
from launch import LaunchDescription
from launch_ros.actions import Node
import launch_testing.markers

import matplotlib  # noqa: F401
import matplotlib.pyplot as plt
import numpy as np  # noqa: F401
import pandas as pd

plt.switch_backend('agg')


def _cleanUpLogs(log_pattern):
    for log in glob(log_pattern):
        os.remove(log)


def _raw_to_jenkins(dataframe, dataframe_perf, csv_path, mode):
    """
    Convert from the raw csv data to formats consumble by Jenkins plugins.

    Do not change the order of the columns in the csv file. The plot plugin
    indexes into the csv using the column number instead of the column name,
    because we're using the columns to identify which test produced the data.

    Changing the column names here will change the name of the line that
    appears on the plot.
    """
    dataframe_agg = dataframe.agg(['median', 'mean'])
    dataframe_agg_perf = dataframe_perf.agg(['max', 'mean', 'sum', 'median'])

    values = [
        dataframe_agg.loc['mean', 'virtual memory (Mb)'],
        dataframe_agg.loc['median', 'virtual memory (Mb)'],
        dataframe['virtual memory (Mb)'].describe(percentiles=[0.95]).iloc[5],
        dataframe_agg.loc['mean', 'cpu_usage (%)'],
        dataframe_agg.loc['median', 'cpu_usage (%)'],
        dataframe['cpu_usage (%)'].describe(percentiles=[0.95]).iloc[5],
        dataframe_agg.loc['mean', 'physical memory (Mb)'],
        dataframe_agg.loc['median', 'physical memory (Mb)'],
        dataframe['physical memory (Mb)'].describe(percentiles=[0.95]).iloc[5],
        dataframe_agg.loc['mean', 'resident anonymous memory (Mb)'],
        dataframe_agg.loc['median', 'resident anonymous memory (Mb)'],
        dataframe['resident anonymous memory (Mb)'].describe(percentiles=[0.95]).iloc[5],
        dataframe_agg_perf.loc['mean', 'latency_mean (ms)'],
        dataframe_agg_perf.loc['median', 'latency_mean (ms)'],
        dataframe_agg_perf['latency_mean (ms)'].describe(percentiles=[0.95]).iloc[5],
        dataframe_agg_perf.loc['max', 'ru_maxrss'],
        dataframe_agg_perf.loc['mean', 'received'],
        dataframe_agg_perf.loc['mean', 'sent'],
        dataframe_agg_perf.loc['sum', 'lost'],
        dataframe_agg.loc['mean', 'system_cpu_usage (%)'],
        dataframe_agg.loc['mean', 'system virtual memory (Mb)'],
    ]

    full_csv_path = '%s_%s.csv' % (csv_path[:-4], mode)
    write_jenkins_plot_csv(
        full_csv_path,
        'Publisher-@TEST_NAME@_Subscriber-@RMW_IMPLEMENTATION_SUB@', values)

    json_path = os.path.splitext(full_csv_path)[0] + '.benchmark.json'
    json_values = {
        'parameters': {
            'runtime': {
                'value': '@PERF_TEST_RUNTIME@',
                'unit': 's',
            },
            'process_count': {
                'value': '2',
            },
            'message_type': {
                'value': '@PERF_TEST_TOPIC@',
            },
        },
        'cpu_usage_' + mode: {
            'dblValue': values[5],
            'unit': 'percent',
        },
        'memory_physical_' + mode: {
            'dblValue': values[8],
            'unit': 'MB',
        },
        'resident_anonymous_memory_' + mode: {
            'dblValue': values[11],
            'unit': 'MB',
        },
        'virtual_memory_' + mode: {
            'dblValue': values[2],
            'unit': 'MB',
        },
    }

    if mode == 'pub':
        json_values.update({
            'lost_messages': {
                'intValue': values[18],
            },
            'received_messages': {
                'dblValue': values[16],
                'unit': 'msg/s',
            },
            'sent_messages': {
                'dblValue': values[17],
                'unit': 'msg/s',
            },
            'average_round_trip_time': {
                'dblValue': values[14],
                'unit': 'ms',
            },
        })

    write_jenkins_benchmark_json(
        json_path,
        'buildfarm_perf_tests.pub_sub',
        {'Publisher-@TEST_NAME@_Subscriber-@RMW_IMPLEMENTATION_SUB@': json_values})


def _raw_to_png(dataframe, dataframe_perf, png_path, mode, rmw_impl):
    pd.options.display.float_format = '{:.4f}'.format

    dataframe[['T_experiment', 'virtual memory (Mb)']].plot(x='T_experiment')
    plt.title('Simple Pub/Sub virtual memory usage\n' + mode + ': ' + rmw_impl)
    plt.ylim(0, 1024)
    plt.savefig(png_path + '_'
                + 'Publisher-@RMW_IMPLEMENTATION@_Subscriber-@RMW_IMPLEMENTATION_SUB@_'
                + mode + '_virtual_memory.png')

    dataframe[['T_experiment', 'cpu_usage (%)']].plot(x='T_experiment')
    plt.title('Simple Pub/Sub CPU usage\n' + mode + ': ' + rmw_impl)
    plt.ylim(0, 100)
    plt.savefig(png_path + '_'
                + 'Publisher-@RMW_IMPLEMENTATION@_Subscriber-@RMW_IMPLEMENTATION_SUB@_'
                + mode + '_cpu_usage.png')

    dataframe[['T_experiment', 'physical memory (Mb)']].plot(x='T_experiment')
    plt.title('Simple Pub/Sub physical memory usage\n' + mode + ': ' + rmw_impl)
    plt.ylim(0, 100)
    plt.savefig(png_path + '_'
                + 'Publisher-@RMW_IMPLEMENTATION@_Subscriber-@RMW_IMPLEMENTATION_SUB@_'
                + mode + '_physical_memory.png')

    dataframe[['T_experiment', 'resident anonymous memory (Mb)']].plot(x='T_experiment')
    plt.title('Simple Pub/Sub resident anonymous memory\n' + mode + ': ' + rmw_impl)
    plt.ylim(0, 100)
    plt.savefig(png_path + '_'
                + 'Publisher-@RMW_IMPLEMENTATION@_Subscriber-@RMW_IMPLEMENTATION_SUB@_'
                + mode + '_resident_anonymous_memory.png')

    if mode == 'subscriber':
        return

    dataframe = dataframe_perf.copy()
    pd.options.display.float_format = '{:.4f}'.format
    dataframe.plot(kind='bar', y=['received', 'sent', 'lost'])
    plt.title('Simple Pub/Sub Received/Sent/Lost messages\n' + mode + ': ' + rmw_impl)
    plt.savefig(png_path + '_'
                + 'Publisher-@RMW_IMPLEMENTATION@_Subscriber-@RMW_IMPLEMENTATION_SUB@_'
                + mode + '_histogram.png')

    dataframe['maxrss (Mb)'] = dataframe['ru_maxrss'] / 1e3
    dataframe.drop(list(dataframe.filter(regex='ru_')), axis=1, inplace=True)
    dataframe['latency_variance (ms) * 100'] = 100.0 * dataframe['latency_variance (ms)']
    dataframe[['T_experiment',
               'latency_min (ms)',
               'latency_max (ms)',
               'latency_mean (ms)',
               'latency_variance (ms) * 100',
               'maxrss (Mb)']].plot(x='T_experiment', secondary_y=['maxrss (Mb)'])
    plt.title('Simple Pub/Sub latency\n' + mode + ': ' + '@RMW_IMPLEMENTATION@'
              + '\nsubscriber: ' + '@RMW_IMPLEMENTATION_SUB@')
    plt.savefig(png_path + '_'
                + 'Publisher-@RMW_IMPLEMENTATION@_Subscriber-@RMW_IMPLEMENTATION_SUB@_'
                + mode + '_latency.png')


@launch_testing.markers.keep_alive
def generate_test_description():
    performance_log_cpumem_pub = tempfile.mkstemp(
        prefix='overhead_cpumem_pub_test_results_'
               + '@TEST_NAME_PUB_SUB@_',
        suffix='.csv', text=True)[1]
    performance_log_cpumem_sub = tempfile.mkstemp(
        prefix='overhead_cpumem_sub_test_results_'
               + '@TEST_NAME_PUB_SUB@_',
        suffix='.csv', text=True)[1]
    performance_log_prefix_pub = tempfile.mkstemp(
        prefix='overhead_pub_test_results_'
               + '@TEST_NAME_PUB_SUB@_',
        text=True)[1]
    performance_log_prefix_sub = tempfile.mkstemp(
        prefix='overhead_sub_test_results_'
               + '@TEST_NAME_PUB_SUB@_',
        text=True)[1]

    # perf_test will create its own logs with this prefix
    # we just need the prefix to be unique
    os.remove(performance_log_prefix_pub)
    os.remove(performance_log_prefix_sub)

    node_main_test = Node(
        package='performance_test', executable='perf_test', output='log',
        arguments=[
            '-c', 'ROS2',
            '-t', '@PERF_TEST_TOPIC@',
            '--max_runtime', '@PERF_TEST_RUNTIME@',
            '-l', performance_log_prefix_pub,
            '--rate', '5',
            '--roundtrip_mode', 'Main',
            '--ignore', '3',
        ] + ([
            '--disable-async',
        ] if '@SYNC_MODE@' == 'sync' and '@COMM@' != 'ROS2' else []),
    )

    system_metric_collector_pub = SystemMetricCollector(
        target_action=node_main_test,
        log_file=performance_log_cpumem_pub,
        timeout=int(@PERF_TEST_RUNTIME@ / (1 / 1.5)))

    node_relay_test = Node(
        package='performance_test', executable='perf_test', output='log',
        arguments=[
            '-c', 'ROS2',
            '-t', '@PERF_TEST_TOPIC@',
            '--max_runtime', '@PERF_TEST_RUNTIME@',
            '-l', performance_log_prefix_sub,
            '--rate', '5',
            '--roundtrip_mode', 'Relay',
            '--ignore', '3',
        ] + ([
            '--disable-async',
        ] if '@SYNC_MODE@' == 'sync' and '@COMM@' != 'ROS2' else []),
        additional_env={'RMW_IMPLEMENTATION': '@RMW_IMPLEMENTATION_SUB@'},
    )

    system_metric_collector_sub = SystemMetricCollector(
        target_action=node_relay_test,
        log_file=performance_log_cpumem_sub,
        timeout=int(@PERF_TEST_RUNTIME@ / (1 / 1.5)))

    nodes = [
        system_metric_collector_pub,
        system_metric_collector_sub,
        node_main_test,
        node_relay_test,
    ]

    return (
        LaunchDescription(nodes + [launch_testing.actions.ReadyToTest()]),
        locals())


class PerformanceTestResults(unittest.TestCase):

    def test_results_@TEST_NAME_PUB_SUB@(
            self, nodes,
            performance_log_cpumem_pub, performance_log_cpumem_sub,
            performance_log_prefix_pub, performance_log_prefix_sub):
        self.addCleanup(_cleanUpLogs, performance_log_prefix_pub + '*')
        self.addCleanup(_cleanUpLogs, performance_log_prefix_sub + '*')
        self.addCleanup(_cleanUpLogs, performance_log_cpumem_pub)
        self.addCleanup(_cleanUpLogs, performance_log_cpumem_sub)

        proc_info = self.proc_info

        timeout = @PERF_TEST_RUNTIME@ * 2
        for process in nodes:
            assert_wait_for_successful_exit(proc_info, process, timeout)

        results_base_path = os.environ.get('PERF_TEST_RESULTS_BASE_PATH')
        performance_overhead_png = os.environ.get('PERFORMANCE_OVERHEAD_PNG')
        if results_base_path:
            performance_logs = glob(performance_log_prefix_pub + '*')
            assert len(performance_logs) == 1
            performance_data = read_performance_test_csv(performance_logs[0])
            performance_data_cpumem = read_performance_test_csv(
                performance_log_cpumem_pub, start_marker=None)
            _raw_to_jenkins(
                performance_data_cpumem, performance_data,
                results_base_path + '.csv', 'pub')
            if performance_overhead_png:
                _raw_to_png(
                    performance_data_cpumem, performance_data,
                    performance_overhead_png, 'publisher', '@RMW_IMPLEMENTATION@')

            performance_logs = glob(performance_log_prefix_sub + '*')
            assert len(performance_logs) == 1
            performance_data = read_performance_test_csv(performance_logs[0])
            performance_data_cpumem = read_performance_test_csv(
                performance_log_cpumem_sub, start_marker=None)
            _raw_to_jenkins(
                performance_data_cpumem, performance_data,
                results_base_path + '.csv', 'sub')
            if performance_overhead_png:
                _raw_to_png(
                    performance_data_cpumem, performance_data,
                    performance_overhead_png, 'subscriber', '@RMW_IMPLEMENTATION_SUB@')
        else:
            print(
                'No results reports written - set PERF_TEST_RESULTS_BASE_PATH to write reports',
                file=sys.stderr)
