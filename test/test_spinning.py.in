# generated from buildfarm_perf_tests/test/test_spinning.py.in
# generated code does not contain a copyright notice

from glob import glob
import os
import sys
import tempfile
import unittest

from buildfarm_perf_tests.launch import attach_system_metric_collector
from buildfarm_perf_tests.test_results import read_performance_test_csv
from buildfarm_perf_tests.test_results import write_jenkins_plot_csv
from launch import LaunchDescription
from launch.actions import EmitEvent
from launch.actions import OpaqueFunction
from launch.actions import RegisterEventHandler
from launch.actions import TimerAction
from launch.event_handlers import OnProcessStart
from launch.events import matches_action
from launch.events.process import ShutdownProcess
from launch_ros.actions import Node
import launch_testing
import matplotlib  # noqa: F401
import matplotlib.pyplot as plt
import numpy as np  # noqa: F401
import pandas as pd

plt.switch_backend('agg')


def _cleanUpLogs(node_spinning_log_prefix):
    for log in glob(node_spinning_log_prefix + '*'):
        os.remove(log)


def _raw_to_csv(dataframe, csv_path):
    """
    Convert from the raw csv data to the csv for the Jenkins plot plugin.

    Do not change the order of the columns. The plot plugin indexes into the
    csv using the column number instead of the column name, because we're
    using the columns to identify which test produced the data.

    Changing the column names here will change the name of the line that
    appears on the plot.
    """
    dataframe_agg = dataframe.agg(['median', 'mean'])

    values = [
        dataframe_agg.loc['mean', 'virtual memory (Mb)'],
        dataframe_agg.loc['median', 'virtual memory (Mb)'],
        dataframe['virtual memory (Mb)'].describe(percentiles=[0.95]).iloc[5],
        dataframe_agg.loc['mean', 'cpu_usage (%)'],
        dataframe_agg.loc['median', 'cpu_usage (%)'],
        dataframe['cpu_usage (%)'].describe(percentiles=[0.95]).iloc[5],
        dataframe_agg.loc['mean', 'physical memory (Mb)'],
        dataframe_agg.loc['median', 'physical memory (Mb)'],
        dataframe['physical memory (Mb)'].describe(percentiles=[0.95]).iloc[5],
        dataframe_agg.loc['mean', 'resident anonymous memory (Mb)'],
        dataframe_agg.loc['median', 'resident anonymous memory (Mb)'],
        dataframe['resident anonymous memory (Mb)'].describe(percentiles=[0.95]).iloc[5],
    ]

    write_jenkins_plot_csv(csv_path, '@TEST_NAME@', values)


def _raw_to_png(dataframe, png_path):
    png_base, png_ext = os.path.splitext(png_path)
    pd.options.display.float_format = '{:.4f}'.format
    dataframe[['T_experiment', 'virtual memory (Mb)']].plot(x='T_experiment')

    plt.title('Node spinning virtual memory usage\n@TEST_NAME@')
    plt.ylim(0, 1024)
    plt.savefig(png_base + '_virtual_memory' + png_ext)

    dataframe[['T_experiment', 'cpu_usage (%)']].plot(x='T_experiment')
    plt.title('Node spinning CPU usage\n@TEST_NAME@')
    plt.ylim(0, 100)
    plt.savefig(png_base + '_cpu_usage' + png_ext)

    dataframe[['T_experiment', 'physical memory (Mb)']].plot(x='T_experiment')
    plt.ylim(0, 100)
    plt.title('Node spinning physical memory usage\n@TEST_NAME@')
    plt.savefig(png_base + '_physical_memory' + png_ext)

    dataframe[['T_experiment', 'resident anonymous memory (Mb)']].plot(x='T_experiment')
    plt.title('Node spinning resident anonymous memory\n@TEST_NAME@')
    plt.ylim(0, 100)
    plt.savefig(png_base + '_resident_anonymous_memory' + png_ext)


def generate_test_description(ready_fn):
    performance_log_prefix_cpumem = tempfile.mkstemp(
        prefix='overhead_node_test_results_@TEST_NAME@_', text=True)[1]

    node_spinning_test = Node(
        package='buildfarm_perf_tests',
        node_executable='node_spinning',
        output='log',
    )
    node_spinning_timer = RegisterEventHandler(OnProcessStart(
        target_action=node_spinning_test, on_start=TimerAction(
            period=float(@PERF_TEST_RUNTIME@), actions=[
                EmitEvent(
                    event=ShutdownProcess(
                        process_matcher=matches_action(node_spinning_test)))
                ])))

    node_metrics_collector, node_spinning_test_hook = attach_system_metric_collector(
        node_spinning_test, performance_log_prefix_cpumem, timeout=@PERF_TEST_RUNTIME@)

    return LaunchDescription([
        node_spinning_test_hook,
        node_spinning_test,
        node_spinning_timer,
        launch_testing.util.KeepAliveProc(),
        OpaqueFunction(function=lambda context: ready_fn()),
    ]), locals()


class NodeSpinningTestTermination(unittest.TestCase):

    def test_termination_@TEST_NAME@(
            self, node_spinning_test, node_metrics_collector, proc_info):
        proc_info.assertWaitForShutdown(process=node_spinning_test,
                                        timeout=(@PERF_TEST_RUNTIME@ * 2))
        proc_info.assertWaitForShutdown(process=node_metrics_collector,
                                        timeout=(@PERF_TEST_RUNTIME@ * 2))


@launch_testing.post_shutdown_test()
class NodeSpinningTestResults(unittest.TestCase):

    def test_results_@TEST_NAME@(
            self, performance_log_prefix_cpumem,
            node_metrics_collector, proc_info):
        self.addCleanup(_cleanUpLogs, performance_log_prefix_cpumem)

        launch_testing.asserts.assertExitCodes(
            proc_info,
            [launch_testing.asserts.EXIT_OK],
            node_metrics_collector,
        )

        results_base_path = os.environ.get('PERF_TEST_RESULTS_BASE_PATH')
        if results_base_path:
            performance_logs_cpumem = glob(performance_log_prefix_cpumem + '*')
            if performance_logs_cpumem:
                performance_data_cpumem = read_performance_test_csv(
                    performance_logs_cpumem[0], start_marker=None)
                _raw_to_csv(performance_data_cpumem, results_base_path + '.csv')
                _raw_to_png(performance_data_cpumem, results_base_path + '.png')
            else:
                print(
                    'No report written - no performance log was produced',
                    file=sys.stderr)
        else:
            print(
                'No results reports written - set PERF_TEST_RESULTS_BASE_PATH to write reports',
                file=sys.stderr)
